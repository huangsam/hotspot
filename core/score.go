package core

import (
	"maps"
	"math"
	"path/filepath"
	"slices"
	"strings"

	"github.com/huangsam/hotspot/schema"
)

// getWeightsForMode returns the weight map for a given scoring mode.
// If custom weights are provided for the mode, they override the defaults.
func getWeightsForMode(mode schema.ScoringMode, customWeights map[schema.ScoringMode]map[schema.BreakdownKey]float64) map[schema.BreakdownKey]float64 {
	// Start with default weights
	defaultWeights := schema.GetDefaultWeights(mode)

	// Override with custom weights if provided
	weights := make(map[schema.BreakdownKey]float64)
	maps.Copy(weights, defaultWeights)

	if customWeights != nil {
		if modeWeights, ok := customWeights[mode]; ok {
			maps.Copy(weights, modeWeights)
		}
	}

	return weights
}

// computeScore calculates a file's importance score (0-100) based on its metrics.
// Supports four core scoring modes:
// - hot: Activity hotspots (high commits, churn, contributors)
// - risk: Knowledge risk/bus factor (few contributors, high inequality)
// - complexity: Technical debt candidates (large, old, high total churn)
// - stale: Maintenance debt (important but untouched)
func computeScore(m *schema.FileResult, mode schema.ScoringMode, customWeights map[schema.ScoringMode]map[schema.BreakdownKey]float64) float64 {
	// DEFENSIVE CHECK: If the file has no content, its score should be 0.
	if m.SizeBytes == 0 {
		return 0.0
	}

	// Tunable maxima to normalize metrics.
	const (
		maxContrib = 20.0    // contributors beyond this saturate
		maxCommits = 500.0   // commits beyond this saturate
		maxSizeKB  = 500.0   // file size in KB beyond this saturate
		maxAgeDays = 3650.0  // ~10 years
		maxChurn   = 5000.0  // total added+deleted lines
		maxRecent  = 50.0    // 50 recent commits is high activity
		maxLOC     = 10000.0 // Lines of Code beyond this saturate (10k lines)
	)

	clamp01 := func(v float64) float64 {
		if v < 0 {
			return 0
		}
		if v > 1 {
			return 1
		}
		return v
	}

	// --- Normalized Metrics [0,1] ---
	nContrib := clamp01(float64(m.UniqueContributors) / maxContrib)
	nCommits := clamp01(float64(m.Commits) / maxCommits)
	nSize := clamp01((float64(m.SizeBytes) / 1024.0) / maxSizeKB)
	nAge := clamp01(math.Log1p(float64(m.AgeDays)) / math.Log1p(maxAgeDays))
	nChurn := clamp01(float64(m.Churn) / maxChurn)
	nLOC := clamp01(float64(m.LinesOfCode) / maxLOC) // Normalized Lines of Code

	// Inverted Metrics
	nGiniRaw := clamp01(m.Gini)            // Gini (raw: high is bad)
	nInvContrib := clamp01(1.0 - nContrib) // Inverse Contributors (high is bad/risky)
	nRecentCommits := clamp01(float64(m.RecentCommits) / maxRecent)
	nInvRecentCommits := clamp01(1.0 - nRecentCommits) // Inverse Recent Activity (high is stale)

	// --------------------------------

	breakdown := make(map[schema.BreakdownKey]float64)
	var raw float64

	// Get weights for the mode
	weights := getWeightsForMode(mode, customWeights)

	// Build breakdown by applying weights to normalized metrics
	breakdown[schema.BreakdownAge] = weights[schema.BreakdownAge] * nAge
	breakdown[schema.BreakdownChurn] = weights[schema.BreakdownChurn] * nChurn
	breakdown[schema.BreakdownCommits] = weights[schema.BreakdownCommits] * nCommits
	breakdown[schema.BreakdownContrib] = weights[schema.BreakdownContrib] * nContrib
	breakdown[schema.BreakdownSize] = weights[schema.BreakdownSize] * nSize

	// Mode-specific metrics
	switch mode {
	case schema.RiskMode:
		breakdown[schema.BreakdownGini] = weights[schema.BreakdownGini] * nGiniRaw
		breakdown[schema.BreakdownInvContrib] = weights[schema.BreakdownInvContrib] * nInvContrib
		breakdown[schema.BreakdownLOC] = weights[schema.BreakdownLOC] * nLOC
	case schema.ComplexityMode:
		breakdown[schema.BreakdownLOC] = weights[schema.BreakdownLOC] * nLOC
		breakdown[schema.BreakdownLowRecent] = weights[schema.BreakdownLowRecent] * nInvRecentCommits
	case schema.StaleMode:
		breakdown[schema.BreakdownInvRecent] = weights[schema.BreakdownInvRecent] * nInvRecentCommits
	}

	for _, value := range breakdown {
		raw += value
	}
	score := raw * 100.0

	pathLower := strings.ToLower(m.Path)
	extLower := strings.ToLower(filepath.Ext(m.Path))

	// Debuff signals on tests, autogenerated code and example code
	if strings.Contains(pathLower, "test") || strings.Contains(pathLower, "generate") || strings.Contains(pathLower, "example") {
		if mode == schema.RiskMode {
			score *= 0.75
		} else {
			score *= 0.50
		}
	}

	// Debuff signals on configuration
	if slices.Contains([]string{"yml", "yaml", "json", "toml", "cfg"}, extLower) {
		if mode == schema.ComplexityMode {
			score *= 0.50
		}
	}

	// Save breakdown (scaled to percent contributions) in the metrics for explain mode.
	if m.Breakdown == nil {
		m.Breakdown = make(map[schema.BreakdownKey]float64)
	}
	for k, v := range breakdown {
		m.Breakdown[k] = v * 100.0
	}

	return score
}

// gini calculates the Gini coefficient for a set of values.
// The Gini coefficient measures inequality in a distribution, ranging from 0 (perfect equality)
// to 1 (perfect inequality). It's used here to measure how evenly distributed commits are
// among contributors.
func gini(values []float64) float64 {
	n := len(values)
	if n == 0 {
		return 0
	}

	var sum float64
	for _, v := range values {
		sum += v
	}
	mean := sum / float64(n)
	if mean == 0 {
		return 0
	}

	var diffSum float64
	for i := range n {
		for j := range n {
			diffSum += math.Abs(values[i] - values[j])
		}
	}

	g := diffSum / (2 * float64(n*n) * mean)
	return math.Min(math.Max(g, 0), 1) // clamp to [0,1]
}

// computeFolderScore computes the final score for a folder as a weighted average.
// The weight for the average is Lines of Code (LOC).
func computeFolderScore(folderResult *schema.FolderResult) float64 {
	// Calculate Weighted Average Score
	if folderResult.TotalLOC == 0 {
		return 0.0
	}
	// Weighted Average Score = SUM(FileScore * FileLOC) / SUM(FileLOC)
	score := folderResult.WeightedScoreSum / float64(folderResult.TotalLOC)

	// Apply optional debuffs if needed, similar to CalculateFileScore
	// For simplicity, we just return the raw weighted average here.
	return score
}
